{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9985735d",
   "metadata": {},
   "source": [
    "# Load and Test the Saved Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19edb9dc",
   "metadata": {},
   "source": [
    "The model weights was saved by calling torch.save(model.state_dict(), 'model path'). Load it and use it to test new images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87eeea",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=32768, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import import_ipynb\n",
    "from SimpleCNN import SimpleCNN\n",
    "\n",
    "model_path = '../models/catsvsDogsTorchModel.pth'\n",
    "model = SimpleCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3260ddd",
   "metadata": {},
   "source": [
    "## Prompt user to enter image path to test the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa06594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user to enter the image path\n",
    "\n",
    "path = input('Enter the image file path:')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762260f",
   "metadata": {},
   "source": [
    "## Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c257e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tranform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     _, predicted = torch.max(output, \u001b[32m1\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names[predicted.item()]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpredict_image\u001b[39m\u001b[34m(img_path)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_image\u001b[39m(img_path):\n\u001b[32m      4\u001b[39m     image = Image.open(img_path)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     image = \u001b[43mtranform\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mval\u001b[39m\u001b[33m'\u001b[39m](image).unsqueeze(\u001b[32m0\u001b[39m).to(device)\n\u001b[32m      6\u001b[39m     output = model(image)\n\u001b[32m      7\u001b[39m     _, predicted = torch.max(output, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'tranform' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "def predict_image(img_path):\n",
    "    image = Image.open(img_path)\n",
    "    tranform = {   \n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    }\n",
    "    class_names = image_datasets['train'].classes\n",
    "    image = tranform['val'](image).unsqueeze(0).to(device)\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    print(f\"Prediction: {class_names[predicted.item()]}\")\n",
    "\n",
    "predict_image(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
